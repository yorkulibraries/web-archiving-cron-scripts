#!/usr/bin/env bash

DATE=$(date +"%Y%m%d")
LOG_FILE="/var/log/heritrix/yu-social-media-browsertrix.log"
CRAWL_CONFIG="/opt/web-archiving-cron-scripts/crawl-configs/yu-social-media.yaml"
CRAWL_DIR="/mnt/omega/web-archives/browsertrix"
COLLECTION_NAME="yu-social-media"
DEDUP_DIR="/mnt/omega/dedup"
WORKERS=10
VERSION="1.5.8"

function start_crawl {
    cd "$CRAWL_DIR" || exit
    /usr/bin/docker run -v "$CRAWL_CONFIG:/app/crawl-config.yaml" -v "$CRAWL_DIR:/crawls" webrecorder/browsertrix-crawler:"$VERSION" crawl --config /app/crawl-config.yaml --collection "$COLLECTION_NAME" --allowHashUrls --workers "$WORKERS" --saveState always --statsFilename "$COLLECTION_NAME-stats-$DATE.json" "+YorkUniversityLibrariesCrawlerBot, diginit@yorku.ca" > "$LOG_FILE" 2>&1

    if [ $? -eq 0 ]; then
        echo "[INFO] $(date) - started" >> "$LOG_FILE"
    else
        echo "[ERROR] $(date)" >> "$LOG_FILE"
    fi
}

function stage_warcs {
    cd "$CRAWL_DIR/collections/$COLLECTION_NAME/archive" || exit
    mv -v ./*.warc.gz "$DEDUP_DIR/input/social-media"
    echo "[INFO] $(date) - staged files for deduplication" >> "$LOG_FILE"
    warc dedup  "$DEDUP_DIR/input/social-media" --prefix "social-media-" -R --keep-index --index-dir "$DEDUP_DIR/index/social-media" --output-dir "$DEDUP_DIR/output/social-media"
    echo "[INFO] $(date) - deduped files" >> "$LOG_FILE"
    mv -v "$DEDUP_DIR/output/social-media"/*.warc.gz "/mnt/omega/web-archives/import"
    echo "[INFO] $(date) - staged files for import" >> "$LOG_FILE"
    rm -v "$DEDUP_DIR"/input/social-media/*.warc.gz
    echo "[INFO] $(date) - removed original files" >> "$LOG_FILE"
}

start_crawl
stage_warcs

docker container prune -f
