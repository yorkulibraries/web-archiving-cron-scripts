#!/usr/bin/env bash

DATE=$(date +"%Y%m%d")
LOG_FILE="/var/log/heritrix/yu-yul-browsertrix.log"
CRAWL_CONFIG="/opt/web-archiving-cron-scripts/crawl-configs/yu-yul.yaml"
CRAWL_DIR="/mnt/omega/web-archives/browsertrix"
COLLECTION_NAME="yu-yul"
WORKERS=8

function start_crawl {
    cd "$CRAWL_DIR"
    /usr/bin/docker run -v "$CRAWL_CONFIG:/app/crawl-config.yaml" -v "$CRAWL_DIR:/crawls" webrecorder/browsertrix-crawler:0.12.0-beta.2 crawl --config /app/crawl-config.yaml --collection "$COLLECTION_NAME" --allowHashUrls --workers "$WORKERS" --saveState always --statsFilename "$COLLECTION_NAME-stats-$DATE.json" "+YorkUniversityLibrariesCrawlerBot, diginit@yorku.ca" > "$LOG_FILE" 2>&1

    if [ $? -eq 0 ]; then
        echo "[INFO] $(date) - started" >> "$LOG_FILE"
    else
        echo "[ERROR] $(date)" >> "$LOG_FILE"
    fi
}

function stage_warcs {
    cd "$CRAWL_DIR/collections/$COLLECTION_NAME/archive"
    rename -v 's/rec-/YORK-LIBRARIES-rec-/g' *.warc.gz
    echo "[INFO] $(date) - renamed files" >> "$LOG_FILE"
    mv -v *.warc.gz /mnt/omega/web-archives/import
    echo "[INFO] $(date) - staged files" >> "$LOG_FILE"
}

start_crawl
stage_warcs
