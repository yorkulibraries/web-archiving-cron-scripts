#!/usr/bin/env bash

DATE=$(date +"%Y%m%d")
LOG_FILE="/var/log/heritrix/yu-yfile-browsertrix.log"
CRAWL_CONFIG="/opt/web-archiving-cron-scripts/crawl-configs/yu-yfile.yaml"
CRAWL_DIR="/mnt/omega/web-archives/browsertrix"
COLLECTION_NAME="yu-yfile"
DEDUP_DIR="/mnt/omega/dedup"
WORKERS=8
VERSION="1.5.8"

function start_crawl {
    cd "$CRAWL_DIR" || exit
    /usr/bin/docker run -v "$CRAWL_CONFIG:/app/crawl-config.yaml" -v "$CRAWL_DIR:/crawls" webrecorder/browsertrix-crawler:"$VERSION" crawl --config /app/crawl-config.yaml --collection "$COLLECTION_NAME" --allowHashUrls --workers "$WORKERS" --saveState always --statsFilename "$COLLECTION_NAME-stats-$DATE.json" "+YorkUniversityLibrariesCrawlerBot, diginit@yorku.ca" > "$LOG_FILE" 2>&1

    if [ $? -eq 0 ]; then
        echo "[INFO] $(date) - started" >> "$LOG_FILE"
    else
        echo "[ERROR] $(date)" >> "$LOG_FILE"
    fi
}

function stage_warcs {
    cd "$CRAWL_DIR/collections/$COLLECTION_NAME/archive" || exit
    mv -v ./*.warc.gz "$DEDUP_DIR/input/yfile"
    echo "[INFO] $(date) - staged files for deduplication" >> "$LOG_FILE"
    warc dedup  "$DEDUP_DIR/input/yfile" --prefix "yfile-" --keep-index --index-dir "$DEDUP_DIR/index/yfile" --output-dir "$DEDUP_DIR/output/yfile"
    echo "[INFO] $(date) - deduped files" >> "$LOG_FILE"
    mv -v "$DEDUP_DIR/output/yfile"/*.warc.gz "/mnt/omega/web-archives/import"
    echo "[INFO] $(date) - staged files for import" >> "$LOG_FILE"
}

start_crawl
stage_warcs

docker container prune -f
