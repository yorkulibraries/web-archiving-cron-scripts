#!/usr/bin/env bash

DATE=$(date +"%Y%m%d")
LOG_FILE="/var/log/heritrix/yu-crs-browsertrix.log"
CRAWL_CONFIG="/opt/web-archiving-cron-scripts/crawl-configs/yu-crs.yaml"
CRAWL_DIR="/mnt/omega/web-archives/browsertrix"
COLLECTION_NAME="yu-crs"
WORKERS=12

function start_crawl {
    cd "$CRAWL_DIR"
    docker run -v "$CRAWL_CONFIG:/app/crawl-config-yu-crs.yaml" -v "$CRAWL_DIR:/crawls" -it webrecorder/browsertrix-crawler:0.12.0-beta.2 crawl --config /app/crawl-config-yu-crs.yaml --collection "$COLLECTION_NAME" --allowHashUrls --workers "$WORKERS" --saveState always --statsFilename "yu-crs-stats-$DATE.json" "+YorkUniversityLibrariesCrawlerBot, diginit@yorku.ca"

    if [ $? -eq 0 ]; then
        echo "[INFO] $(date) - started" >> "$LOG_FILE"
    else
        echo "[ERROR] $(date)" >> "$LOG_FILE"
    fi
}

function stage_warcs {
    cd "$CRAWL_DIR/collections/$COLLECTION_NAME/archive"
    rename -v 's/rec-/YU-CRS-rec-/g' *.warc.gz
    mv -v *.warc.gz /mnt/omega/web-archives/import
}

start_crawl
stage_warcs
